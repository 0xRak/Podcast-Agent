"""Weekly podcast digest generator with category-based aggregation."""

import logging
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import re

logger = logging.getLogger(__name__)

class WeeklyDigestGenerator:
    """Generate comprehensive weekly digest reports by category."""

    def __init__(self, config_path: Optional[str] = None):
        """Initialize the weekly digest generator."""
        self.config_path = config_path or "config/channels.yaml"
        self.channels_config = self._load_channels_config()

    def _load_channels_config(self) -> Dict[str, Any]:
        """Load channels configuration from YAML file."""
        try:
            config_file = Path(self.config_path)
            if config_file.exists():
                with open(config_file, 'r') as f:
                    return yaml.safe_load(f)
            else:
                logger.warning(f"Config file not found: {self.config_path}")
                return {"channels": {}, "categories": {}}
        except Exception as e:
            logger.error(f"Error loading config: {str(e)}")
            return {"channels": {}, "categories": {}}

    def generate_category_digest(self, category: str, summaries: List[Dict[str, Any]],
                               week_start: datetime, week_end: datetime) -> str:
        """Generate a comprehensive weekly digest for a specific category."""

        # Get category channels
        category_channels = self.channels_config.get("categories", {}).get(category, [])

        # Filter summaries to only include this category
        category_summaries = [s for s in summaries if s.get("channel_id") in category_channels]

        if not category_summaries:
            return self._generate_empty_digest(category, week_start, week_end)

        # Generate the digest content
        digest_content = self._build_category_digest(category, category_summaries, week_start, week_end)

        return digest_content

    def _build_category_digest(self, category: str, summaries: List[Dict[str, Any]],
                             week_start: datetime, week_end: datetime) -> str:
        """Build the comprehensive category digest."""

        category_title = category.upper()
        week_range = f"{week_start.strftime('%B %d')} - {week_end.strftime('%B %d, %Y')}"

        # Calculate statistics
        total_duration = sum(s.get("duration_seconds", 0) for s in summaries)
        total_episodes = len(summaries)
        channels_processed = len(set(s.get("channel_name", "Unknown") for s in summaries))

        # Build digest sections
        header = self._build_digest_header(category_title, week_range, summaries,
                                         total_duration, total_episodes, channels_processed)

        executive_summary = self._extract_executive_summary(category, summaries)
        top_insights = self._extract_top_insights(summaries)
        notable_quotes = self._extract_notable_quotes(summaries)
        trending_topics = self._identify_trending_topics(summaries)
        show_highlights = self._create_show_highlights(summaries)

        # Assemble the digest
        digest = f"""{header}

## Executive Summary

{executive_summary}

## Top Insights This Week

{top_insights}

## Notable Quotes

{notable_quotes}

## Trending Topics

{trending_topics}

## Individual Show Highlights

{show_highlights}

## Content Sources

- **{total_episodes} episodes** analyzed from **{channels_processed} shows**
- **Total content duration**: {self._format_duration(total_duration)}
- **Week analyzed**: {week_range}

---
*Weekly digest generated by Podcast Research Tool | {datetime.now().strftime('%B %d, %Y')}*
"""

        return digest

    def _build_digest_header(self, category_title: str, week_range: str,
                           summaries: List[Dict[str, Any]], total_duration: int,
                           total_episodes: int, channels_processed: int) -> str:
        """Build the digest header section."""

        # List the podcasts that were summarized
        podcast_list = []
        for summary in summaries:
            channel_name = summary.get("channel_name", "Unknown")
            episode_title = summary.get("title", "Unknown Episode")
            # Truncate long titles
            if len(episode_title) > 60:
                episode_title = episode_title[:57] + "..."
            podcast_list.append(f"- **{channel_name}**: {episode_title}")

        podcasts_section = "\n".join(podcast_list)

        return f"""# {category_title} Weekly Digest

**Week of {week_range}**

### Podcasts Summarized This Week
{podcasts_section}

### Weekly Overview
- **Episodes processed**: {total_episodes}
- **Shows covered**: {channels_processed}
- **Total content**: {self._format_duration(total_duration)}
- **Key developments**: Major {category.lower()} trends and insights from leading voices
"""

    def _extract_executive_summary(self, category: str, summaries: List[Dict[str, Any]]) -> str:
        """Create an executive summary of the week's key themes."""

        # Extract themes from all summaries
        all_themes = []
        for summary in summaries:
            content = summary.get("content", "")
            themes = self._extract_themes_from_content(content)
            all_themes.extend(themes)

        # Find most common themes
        theme_counts = {}
        for theme in all_themes:
            theme_counts[theme] = theme_counts.get(theme, 0) + 1

        top_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)[:4]

        if not top_themes:
            return f"This week's {category.lower()} content covered various topics with insights from industry leaders."

        # Create executive summary
        main_themes = [theme for theme, _ in top_themes]

        summary = f"This week in {category.lower()}, the conversation centered around "
        if len(main_themes) >= 3:
            summary += f"{', '.join(main_themes[:-1])}, and {main_themes[-1]}. "
        else:
            summary += f"{' and '.join(main_themes)}. "

        summary += f"Across {len(summaries)} episodes, leading voices provided insights into "
        summary += f"market developments, strategic considerations, and emerging trends shaping the {category.lower()} landscape."

        return summary

    def _extract_top_insights(self, summaries: List[Dict[str, Any]]) -> str:
        """Extract and rank the top insights from all summaries."""

        insights = []

        for summary in summaries:
            content = summary.get("content", "")
            channel = summary.get("channel_name", "Unknown")

            # Extract insight-like content
            summary_insights = self._find_insights_in_content(content)
            for insight in summary_insights:
                insights.append({
                    "text": insight,
                    "source": channel,
                    "relevance": len(insight)  # Simple relevance scoring
                })

        # Sort by relevance and deduplicate
        insights.sort(key=lambda x: x["relevance"], reverse=True)
        unique_insights = []
        seen_insights = set()

        for insight in insights:
            # Simple deduplication based on first 50 characters
            key = insight["text"][:50].lower()
            if key not in seen_insights and len(unique_insights) < 5:
                seen_insights.add(key)
                unique_insights.append(insight)

        if not unique_insights:
            return "Key insights from this week's discussions include strategic perspectives on market developments and emerging opportunities."

        # Format insights
        formatted_insights = []
        for i, insight in enumerate(unique_insights, 1):
            formatted_insights.append(f"**{i}. {insight['text']}** *(Source: {insight['source']})*")

        return "\n\n".join(formatted_insights)

    def _extract_notable_quotes(self, summaries: List[Dict[str, Any]]) -> str:
        """Extract the most impactful quotes from all summaries."""

        quotes = []

        for summary in summaries:
            content = summary.get("content", "")
            channel = summary.get("channel_name", "Unknown")

            # Find quoted content
            quote_patterns = [
                r'"([^"]{40,200})"',  # Text in quotes
                r'["""]([^"""]{40,200})["""]',  # Smart quotes
            ]

            for pattern in quote_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if len(match.strip()) > 30:
                        quotes.append({
                            "text": match.strip(),
                            "source": channel
                        })

        # Select best quotes (limit to 4)
        if not quotes:
            return "Notable perspectives from this week's discussions highlighted key market dynamics and strategic considerations."

        unique_quotes = []
        seen_quotes = set()

        for quote in quotes[:10]:  # Check first 10 quotes
            key = quote["text"][:30].lower()
            if key not in seen_quotes and len(unique_quotes) < 4:
                seen_quotes.add(key)
                unique_quotes.append(quote)

        # Format quotes
        formatted_quotes = []
        for quote in unique_quotes:
            formatted_quotes.append(f'> "{quote["text"]}"  \n*â€” {quote["source"]}*')

        return "\n\n".join(formatted_quotes)

    def _identify_trending_topics(self, summaries: List[Dict[str, Any]]) -> str:
        """Identify topics that appeared across multiple shows."""

        # Extract keywords/topics from all summaries
        all_topics = []
        for summary in summaries:
            content = summary.get("content", "")
            topics = self._extract_topics_from_content(content)
            all_topics.extend(topics)

        # Count topic occurrences
        topic_counts = {}
        for topic in all_topics:
            topic_counts[topic] = topic_counts.get(topic, 0) + 1

        # Find topics that appeared in multiple summaries (trending)
        trending = [(topic, count) for topic, count in topic_counts.items() if count > 1]
        trending.sort(key=lambda x: x[1], reverse=True)

        if not trending:
            return "This week's discussions covered diverse topics relevant to current market conditions and strategic developments."

        # Format trending topics
        trending_list = []
        for topic, count in trending[:6]:  # Top 6 trending topics
            shows = "multiple shows" if count > 2 else f"{count} shows"
            trending_list.append(f"- **{topic.title()}** (discussed across {shows})")

        return "\n".join(trending_list)

    def _create_show_highlights(self, summaries: List[Dict[str, Any]]) -> str:
        """Create brief highlights for each individual show."""

        highlights = []

        for summary in summaries:
            channel = summary.get("channel_name", "Unknown")
            title = summary.get("title", "Unknown Episode")
            content = summary.get("content", "")
            duration = self._format_duration(summary.get("duration_seconds", 0))

            # Extract a brief highlight from the content
            highlight = self._extract_show_highlight(content)

            # Format the show highlight
            show_summary = f"""### {channel}
**Episode**: {title} | **Duration**: {duration}

{highlight}"""

            highlights.append(show_summary)

        return "\n\n".join(highlights)

    def _extract_show_highlight(self, content: str) -> str:
        """Extract a concise highlight from show content."""

        # Look for key takeaway patterns
        patterns = [
            r'(?:key takeaway|main point|important insight)[:\s]+([^.!?]{50,200})',
            r'(?:the conclusion|ultimately|in summary)[:\s]+([^.!?]{50,200})',
            r'(?:what\'s interesting|what stands out)[:\s]+([^.!?]{50,200})'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                return matches[0].strip()

        # Fallback: extract first meaningful sentence
        sentences = re.split(r'[.!?]+', content)
        for sentence in sentences:
            if len(sentence.strip()) > 50 and len(sentence.strip()) < 300:
                return sentence.strip()

        # Final fallback
        return "Provided valuable insights and perspectives on current market developments."

    def _extract_themes_from_content(self, content: str) -> List[str]:
        """Extract main themes from content."""

        theme_keywords = {
            'artificial intelligence': ['ai', 'artificial intelligence', 'machine learning', 'llm', 'gpt'],
            'cryptocurrency': ['crypto', 'bitcoin', 'ethereum', 'defi', 'blockchain', 'token'],
            'market analysis': ['market', 'trading', 'price', 'valuation', 'investment'],
            'regulation': ['regulation', 'regulatory', 'compliance', 'legal', 'government'],
            'innovation': ['innovation', 'technology', 'breakthrough', 'development', 'advancement'],
            'strategy': ['strategy', 'approach', 'framework', 'methodology', 'planning'],
        }

        content_lower = content.lower()
        themes = []

        for theme, keywords in theme_keywords.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score >= 2:  # Theme must appear with multiple keywords
                themes.append(theme)

        return themes

    def _extract_topics_from_content(self, content: str) -> List[str]:
        """Extract specific topics mentioned in content."""

        # Common topic patterns
        patterns = [
            r'\b([A-Z][a-z]+(?: [A-Z][a-z]+)*)\b',  # Capitalized terms
            r'\b(bitcoin|ethereum|ai|defi|nft|web3)\b',  # Specific terms
        ]

        topics = set()

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                if len(match) > 3 and len(match) < 50:
                    topics.add(match.lower())

        return list(topics)

    def _find_insights_in_content(self, content: str) -> List[str]:
        """Find insight-like statements in content."""

        insight_patterns = [
            r'(?:key insight|important point|takeaway)[:\s]+([^.!?]{30,200})',
            r'(?:what this means|the implication)[:\s]+([^.!?]{30,200})',
            r'(?:surprisingly|interestingly|notably)[:\s,]+([^.!?]{30,200})',
        ]

        insights = []

        for pattern in insight_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                insight = match.strip()
                if len(insight) > 20:
                    insights.append(insight)

        return insights[:3]  # Limit to 3 insights per summary

    def _format_duration(self, duration_seconds: int) -> str:
        """Format duration in seconds to human-readable string."""
        if duration_seconds == 0:
            return "unknown duration"

        hours, remainder = divmod(duration_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)

        if hours > 0:
            return f"{hours}h {minutes}m"
        elif minutes > 0:
            return f"{minutes}m"
        else:
            return f"{seconds}s"

    def _generate_empty_digest(self, category: str, week_start: datetime, week_end: datetime) -> str:
        """Generate digest when no content is available for the category."""

        week_range = f"{week_start.strftime('%B %d')} - {week_end.strftime('%B %d, %Y')}"

        return f"""# {category.upper()} Weekly Digest

**Week of {week_range}**

## No New Content This Week

No new episodes were found for {category.lower()} podcasts during this time period. This could be due to:

- No new episodes published this week
- Episodes not yet processed
- Temporary access issues

Check back next week for the latest {category.lower()} insights and discussions.

---
*Weekly digest generated by Podcast Research Tool | {datetime.now().strftime('%B %d, %Y')}*
"""

    def generate_master_digest(self, category_digests: Dict[str, str],
                             week_start: datetime, week_end: datetime) -> str:
        """Generate master weekly digest combining all categories."""

        week_range = f"{week_start.strftime('%B %d')} - {week_end.strftime('%B %d, %Y')}"

        # Build master digest
        master_digest = f"""# Weekly Podcast Digest

**Week of {week_range}**

## Overview

This week's podcast analysis covers {len(category_digests)} categories with insights from leading voices across different domains. Below is a comprehensive overview of the week's key discussions and trends.

## Category Summaries

"""

        # Add brief summary for each category
        for category in category_digests.keys():
            master_digest += f"### {category.upper()}\n"
            master_digest += f"Key developments and insights from this week's {category.lower()} podcast discussions.\n\n"

        master_digest += """## Cross-Category Analysis

This week's discussions revealed interconnected themes across different domains, highlighting how developments in one area often impact others.

## Week's Biggest Stories

The most significant developments discussed across all podcasts this week represent important trends worth tracking.

---

*For detailed analysis of each category, see the individual category digest reports.*

---
*Master weekly digest generated by Podcast Research Tool | """ + datetime.now().strftime('%B %d, %Y') + "*"

        return master_digest


# Example usage for testing
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    generator = WeeklyDigestGenerator()

    # Test with sample data
    sample_summaries = [
        {
            "channel_name": "Empire",
            "channel_id": "empirepodcast",
            "title": "Bitcoin ETF Approval and Market Impact",
            "content": "The key insight this week is that Bitcoin ETF approval represents a major institutional shift. What this means is broader crypto adoption. The implication is that we're entering a new phase of cryptocurrency acceptance.",
            "duration_seconds": 3600
        },
        {
            "channel_name": "Bankless",
            "channel_id": "banklesshq",
            "title": "DeFi Protocols and Yield Strategies",
            "content": "Surprisingly, DeFi yields have stabilized despite market volatility. The takeaway is that protocol improvements are working. What's interesting is the correlation between governance and performance.",
            "duration_seconds": 2700
        }
    ]

    week_start = datetime.now() - timedelta(days=7)
    week_end = datetime.now()

    # Generate category digest
    digest = generator.generate_category_digest("crypto", sample_summaries, week_start, week_end)

    print("Generated Category Digest:")
    print("=" * 60)
    print(digest)